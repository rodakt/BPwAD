{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblioteki Pythona w analizie danych\n",
    "\n",
    "## Tomasz Rodak\n",
    "\n",
    "Wykład 5\n",
    "\n",
    "---\n",
    "\n",
    "Literatura:\n",
    "\n",
    "- [PRML](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) Christopher M. Bishop, \"Pattern Recognition and Machine Learning\", 2006.\n",
    "- [PML-1](https://probml.github.io/pml-book/) Kevin P. Murphy, \"Probabilistic Machine Learning: An Introduction\", 2022.\n",
    "- [Dokumentacja NumPy](https://numpy.org/doc/stable/)\n",
    "- [Dokumentacja scikit-learn](https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn - estymatory, transformatory i potoki\n",
    "\n",
    "Biblioteka scikit-learn wprowadza następujące wysokopoziomowe pojęcia:\n",
    "- **estymator** (*estimator*) - obiekt tworzący deterministyczną funkcję na podstawie danych zgodnie z reprezentowanym w nim modelem. \n",
    "- **transformator** (*transformer*) - estymator posiadający metodę `transform()`.\n",
    "- **potok** (*pipeline*) - sekwencja transformatorów i estymatorów, które są stosowane w kolejności. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estymatory i transformatory\n",
    "\n",
    "Estymator to obiekt reprezentujący model. Każdy estymator powien posiadać metodę `fit()`, która przyjmuje dane i uczy model, czyli tworzy deterministyczną funkcję na podstawie danych i zgodnie z reprezentowanym w nim modelem. Estymatory posiadają również metodę `predict()` lub `transform()` i ewentualnie `fit_predict()` lub `fit_transform()`.\n",
    "\n",
    "\n",
    "#### Metoda `fit()`\n",
    "\n",
    "Metoda `fit()` zwykle przyjmuje dwa argumenty:\n",
    "- `X` - dane wejściowe (np. cechy, obrazy, teksty)\n",
    "- `y` - dane wyjściowe (np. etykiety, wartości docelowe)\n",
    "Metoda `fit()` modyfikuje stan estymatora \"w miejscu\" oraz zwraca dopasowany estymator - pozwala to na łączenie wielu wywołań w potoku.\n",
    "\n",
    "Jeśli uczenie jest nienadzorowane, to argument `y` nie jest wymagany, zwykle automatycznie ustawiany jest na wartość `None`.\n",
    "\n",
    "Częstym efektem wywołania metody `fit()` jest utworzenie atrybutów w estymatorze o nazwach kończących się na `_` (np. `coef_`, `intercept_`, `n_features_in_`, `n_classes_`), które przechowują różne informacje o stanie estymatora po wywołaniu metody `fit()`.\n",
    "\n",
    "#### Metoda `predict()`\n",
    "\n",
    "Metoda `predict()` przyjmuje dane wejściowe `X` i zwraca przewidywane wartości wyjściowe `y`. Jest to metoda, która jest wywoływana po `fit()` - model musi być najpierw wyuczony na danych, aby można było przewidywać nowe wartości. \n",
    "\n",
    "#### Metoda `transform()`\n",
    "\n",
    "Metoda `transform()` przyjmuje dane wejściowe `X` i zwraca ich przekształcenie na jakąś nową przestrzeń. Zwrócone dane oznacza się często jako `Xt`. Jest to metoda, która również jest wywoływana po `fit()`.\n",
    "\n",
    "#### Metody `fit_transform()` i `fit_predict()`\n",
    "\n",
    "Metody `fit_transform()` i `fit_predict()` są skrótami dla wywołania `fit()` i następnie `transform()` lub `predict()`.\n",
    "Metoda `fit_transform()` przyjmuje dane wejściowe `X`, uczy estymator na tych danych wywołując `fit()` i zwraca przekształcone dane wyjściowe `Xt` wywołując `transform()`. Podobnie `fit_predict()` przyjmuje dane wejściowe `X`, uczy estymator na tych danych wywołując `fit()` i zwraca przewidywane wartości wyjściowe `y` wywołując `predict()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykłady estymatorów wbudowanych w scikit-learn\n",
    "\n",
    "#### Regresja logistyczna\n",
    "\n",
    "Model regresji logistycznej jest implementowany w `sklearn.linear_model.LogisticRegression`. Jest to przykład estymatora uczenia nadzorowanego. Spośród metod opisanych powyżej posiada:\n",
    "- `fit(X, y)` - uczy model na danych `X` i etykietach `y`. Po wywołaniu tej metody w estymatorze są dostępne atrybuty:\n",
    "  - `coef_` - współczynniki regresji\n",
    "  - `intercept_` - wyraz wolny\n",
    "  - `n_features_in_` - liczba cech w danych wejściowych\n",
    "  - `feature_names_in_` - nazwy cech w danych wejściowych\n",
    "  - `classes_` - etykiety klas\n",
    "  - `n_iter` - liczba iteracji w procesie uczenia\n",
    "- `predict(X)` - przewiduje etykiety klas dla nowych danych `X`\n",
    "\n",
    "Dodatkowo, model ten, tak jak i wiele innych modeli szacujących prawdopodobieństwo przynależności do klas, posiada metodę `predict_proba(X)`, która zwraca prawdopodobieństwa przynależności do klas dla danych `X`.\n",
    "\n",
    "#### K-means\n",
    "\n",
    "Model K-średnich (*K-means*) jest implementowany w `sklearn.cluster.KMeans`. Jest to przykład estymatora uczenia nienadzorowanego. Posiada wszystkie metody opisane powyżej:\n",
    "- `fit(X)` - uczy model na danych `X`. Po wywołaniu tej metody w estymatorze są dostępne atrybuty:\n",
    "  - `cluster_centers_` - współrzędne centroidów klastrów\n",
    "  - `labels_` - etykiety klastrów dla każdego punktu danych\n",
    "  - `inertia_` - suma kwadratów odległości punktów danych do ich centroidów\n",
    "- `predict(X)` - przewiduje etykiety klastrów dla nowych danych `X`\n",
    "- `fit_predict(X)` - uczy model na danych `X` i zwraca etykiety klastrów dla tych danych\n",
    "- `transform(X)` - przekształca dane `X` na przestrzeń odległości do centroidów klastrów\n",
    "- `fit_transform(X)` - uczy model na danych `X` i zwraca `X` przekształcone na przestrzeń odległości do centroidów klastrów.\n",
    "\n",
    "#### PCA\n",
    "\n",
    "Model analizy głównych składowych (*PCA*) jest implementowany w `sklearn.decomposition.PCA`. Jest to przykład estymatora uczenia nienadzorowanego. Spośród metod opisanych powyżej posiada:\n",
    "- `fit(X)` - uczy model na danych `X`. Po wywołaniu tej metody w estymatorze dostępne są m.in. atrybuty:\n",
    "  - `components_` - macierz głównych składowych\n",
    "  - `explained_variance_` - wariancja wyjaśniona przez każdą główną składową\n",
    "  - `singular_values_` - wartości osobliwe\n",
    "- `transform(X)` - przekształca dane `X` na przestrzeń głównych składowych\n",
    "- `fit_transform(X)` - uczy model na danych `X` i zwraca przekształcone dane `X` w przestrzeni głównych składowych\n",
    "\n",
    "Dodatkowo, model ten, tak jak i wiele innych modeli transformujących dane, posiada metodę `inverse_transform(Xt)`, która przekształca dane `Xt` z powrotem do oryginalnej przestrzeni. Ponieważ PCA nie jest transformatorem różnowartościowym (podczas transformacji zmienia się liczba cech i część informacji jest tracona), to metoda `inverse_transform(Xt)` nie zwraca danych oryginalnych, a jedynie ich przybliżenie.\n",
    "\n",
    "#### `MinMaxScaler`\n",
    "\n",
    "Model `MinMaxScaler` jest implementowany w `sklearn.preprocessing.MinMaxScaler`. Jest to przykład nienadzorowanego transformatora. Spośród metod opisanych powyżej posiada:\n",
    "- `fit(X)` - uczy model na danych `X`. Po wywołaniu tej metody w estymatorze dostępne są m.in. atrybuty:\n",
    "  - `data_max_` - maksymalne wartości cech w danych\n",
    "  - `data_min_` - minimalne wartości cech w danych\n",
    "  - `data_range_` - zakres wartości cech w danych\n",
    "  - `n_features_in_` - liczba cech w danych wejściowych\n",
    "  - ...\n",
    "- `transform(X)` - przekształca dane `X` na przestrzeń znormalizowaną do przedziału [0, 1]\n",
    "- `fit_transform(X)` - uczy model na danych `X` i zwraca przekształcone dane `X` w przedziale [0, 1]\n",
    "\n",
    "Dodatkowo, model ten, tak jak i wiele innych modeli transformujących dane, posiada metodę `inverse_transform(Xt)`, która przekształca dane `Xt` z powrotem do oryginalnej przestrzeni.\n",
    "\n",
    "#### `PolynomialFeatures`\n",
    "\n",
    "Model `PolynomialFeatures` jest implementowany w `sklearn.preprocessing.PolynomialFeatures`. Jest to przykład nienadzorowanego transformatora służącego do generowania cech wielomianowych. Spośród metod opisanych powyżej posiada:\n",
    "- `fit(X)` - uczy model na danych `X`. Po wywołaniu tej metody w estymatorze dostępne są m.in. atrybuty:\n",
    "  - `powers_` - wykładniki cech wejściowych w wyjściu\n",
    "  - `n_features_in_` - liczba cech w danych wejściowych\n",
    "  - `n_output_features_` - liczba cech wyjściowych\n",
    "  - ...\n",
    "- `transform(X)` - przekształca dane `X` na przestrzeń cech wielomianowych\n",
    "- `fit_transform(X)` - uczy model na danych `X` i zwraca przekształcone dane `X` w przestrzeni cech wielomianowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typowy schemat wykorzystania estymatora\n",
    "\n",
    "1. Inicjalizacja. Polega na stworzeniu obiektu estymatora, który reprezentuje model. Parametry estymatora są hiperparametrami modelu i są ustalane przed rozpoczęciem uczenia.\n",
    "2. Uczenie. Polega na wywołaniu metody `fit()` na obiekcie estymatora, co prowadzi do dopasowania modelu do danych.\n",
    "3. Przewidywanie lub transformacja. Polega na wywołaniu metody `predict()` lub `transform()` na obiekcie estymatora, co prowadzi do uzyskania przewidywanych wartości wyjściowych lub przekształconych danych wejściowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie własnych estymatorów w scikit-learn\n",
    "\n",
    "Tworzenie własnych estymatorów w scikit-learn wymaga zrozumienia interfejsu API biblioteki oraz konwencji, które ściśle definiują, jak powinien zachowywać się estymator. \n",
    "\n",
    "#### Podstawowe zasady\n",
    "\n",
    "1. **Dziedziczenie po odpowiednich klasach bazowych**:\n",
    "   - `BaseEstimator`: zapewnia funkcje pomocnicze, np. ustawianie parametrów\n",
    "   - Odpowiedni *mixin* w zależności od typu estymatora:\n",
    "     - `ClassifierMixin`: dla klasyfikatorów\n",
    "     - `RegressorMixin`: dla algorytmów regresji\n",
    "     - `TransformerMixin`: dla transformatorów danych\n",
    "     - `ClusterMixin`: dla algorytmów klasteryzacji\n",
    "\n",
    "2. **Implementacja wymaganych metod**:\n",
    "   - `fit(X, y)`: trenuje model na podstawie danych\n",
    "   - Zależnie od typu estymatora:\n",
    "     - Klasyfikatory/regresory: `predict(X)`\n",
    "     - Transformatory: `transform(X)` i opcjonalnie `fit_transform(X, y)`\n",
    "\n",
    "3. **Konwencje nazewnictwa**:\n",
    "   - Parametry jako atrybuty inicjalizacji (np. `self.param = param` w `__init__`)\n",
    "   - Atrybuty wyuczone kończą się podkreślnikiem (np. `self.coef_`)\n",
    "   - Metody opakowujące powinny zwracać `self` dla umożliwienia łańcuchowania\n",
    "\n",
    "#### Szczegółowa implementacja\n",
    "\n",
    "##### 1. Struktura klasy i inicjalizacja\n",
    "\n",
    "```python\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class MojEstymator(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, parametr1=domyslna_wartosc1, parametr2=domyslna_wartosc2):\n",
    "        # Zapisz wszystkie parametry inicjalizacyjne\n",
    "        self.parametr1 = parametr1\n",
    "        self.parametr2 = parametr2\n",
    "```\n",
    "\n",
    "##### 2. Metoda fit\n",
    "\n",
    "```python\n",
    "def fit(self, X, y):\n",
    "    # Walidacja danych wejściowych\n",
    "    X, y = check_X_y(X, y, accept_sparse=True)\n",
    "    \n",
    "    # Zachowaj wymiary danych\n",
    "    self.n_features_in_ = X.shape[1]\n",
    "    \n",
    "    # Zaimplementuj logikę trenowania\n",
    "    # ... (twój algorytm)\n",
    "    \n",
    "    # Zdefiniuj atrybuty wyuczone (z podkreślnikiem)\n",
    "    self.coef_ = ...\n",
    "    self.intercept_ = ...\n",
    "    \n",
    "    # Oznacz, że model został wytrenowany\n",
    "    self.is_fitted_ = True\n",
    "    \n",
    "    # Zwróć self dla umożliwienia łańcuchowania\n",
    "    return self\n",
    "```\n",
    "\n",
    "##### 3. Metoda predict (dla regresorów/klasyfikatorów)\n",
    "\n",
    "```python\n",
    "def predict(self, X):\n",
    "    # Sprawdź, czy model został wytrenowany\n",
    "    check_is_fitted(self, 'is_fitted_')\n",
    "    \n",
    "    # Walidacja danych wejściowych\n",
    "    X = check_array(X, accept_sparse=True)\n",
    "    \n",
    "    # Sprawdź, czy wymiar danych jest zgodny z modelem\n",
    "    if X.shape[1] != self.n_features_in_:\n",
    "        raise ValueError(f\"X ma {X.shape[1]} cech, ale {self.__class__.__name__} \"\n",
    "                         f\"został wytrenowany z {self.n_features_in_} cechami.\")\n",
    "    \n",
    "    # Zastosuj model do predykcji\n",
    "    # ... (twój algorytm)\n",
    "    return y_pred\n",
    "```\n",
    "\n",
    "##### 4. Dla transformatorów - metoda transform\n",
    "\n",
    "```python\n",
    "def transform(self, X):\n",
    "    # Sprawdź, czy model został wytrenowany\n",
    "    check_is_fitted(self, 'is_fitted_')\n",
    "    \n",
    "    # Walidacja danych wejściowych\n",
    "    X = check_array(X, accept_sparse=True)\n",
    "    \n",
    "    # Zastosuj transformację\n",
    "    # ... (twoja logika)\n",
    "    return X_transformed\n",
    "```\n",
    "\n",
    "#### Integracja z ekosystemem scikit-learn\n",
    "\n",
    "Prawidłowo zaimplementowany estymator można:\n",
    "- Używać w potokach (`Pipeline`)\n",
    "- Stosować w walidacji krzyżowej (`cross_val_score`)\n",
    "- Optymalizować za pomocą narzędzi wyszukiwania hiperparametrów (`GridSearchCV`, `RandomizedSearchCV`)\n",
    "- Serializować za pomocą `pickle` lub `joblib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykład\n",
    "\n",
    "Klasa `RegresjaLiniowa` jest przykładem własnego estymatora regresji liniowej zgodnego z API scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class RegresjaLiniowa(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Prosty estymator regresji liniowej zaimplementowany ręcznie.\n",
    "    \n",
    "    Parametry\n",
    "    ---------\n",
    "    fit_intercept : bool, domyślnie=True\n",
    "        Czy dopasować wyraz wolny.\n",
    "    learning_rate : float, domyślnie=0.01\n",
    "        Współczynnik uczenia używany w metodzie gradientu prostego.\n",
    "    n_iterations : int, domyślnie=1000\n",
    "        Liczba iteracji algorytmu gradientu prostego.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fit_intercept=True, learning_rate=0.01, n_iterations=1000):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Dopasowanie estymatora do danych treningowych.\n",
    "        \n",
    "        Parametry\n",
    "        ---------\n",
    "        X : {array-like, sparse matrix} o kształcie (n_próbek, n_cech)\n",
    "            Dane treningowe.\n",
    "        y : array-like o kształcie (n_próbek,)\n",
    "            Wartości docelowe.\n",
    "            \n",
    "        Zwraca\n",
    "        -------\n",
    "        self : obiekt\n",
    "            Zwraca samego siebie.\n",
    "        \"\"\"\n",
    "        # Sprawdzenie poprawności danych wejściowych\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        # Inicjalizacja atrybutów\n",
    "        self.n_samples_, self.n_features_ = X.shape\n",
    "        \n",
    "        # Dodanie kolumny jedynek dla wyrazu wolnego\n",
    "        if self.fit_intercept:\n",
    "            X_b = np.c_[np.ones((self.n_samples_, 1)), X]\n",
    "        else:\n",
    "            X_b = X\n",
    "        \n",
    "        # Inicjalizacja wag (współczynników) jako zera\n",
    "        self.coef_ = np.zeros(X_b.shape[1])\n",
    "        \n",
    "        # Trening za pomocą gradientu prostego\n",
    "        for _ in range(self.n_iterations):\n",
    "            # Przewidywanie\n",
    "            y_pred = np.dot(X_b, self.coef_)\n",
    "            \n",
    "            # Obliczenie błędu\n",
    "            error = y_pred - y\n",
    "            \n",
    "            # Aktualizacja wag\n",
    "            gradients = 2/self.n_samples_ * X_b.T.dot(error)\n",
    "            self.coef_ = self.coef_ - self.learning_rate * gradients\n",
    "        \n",
    "        # Rozdzielenie wyrazu wolnego i współczynników, jeśli fit_intercept=True\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = self.coef_[0]\n",
    "            self.coef_ = self.coef_[1:]\n",
    "        else:\n",
    "            self.intercept_ = 0\n",
    "        \n",
    "        # Oznaczenie, że model został wytrenowany\n",
    "        self.is_fitted_ = True\n",
    "        \n",
    "        # Zwrócenie samego siebie zgodnie z konwencją scikit-learn\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predykcja wartości dla X.\n",
    "        \n",
    "        Parametry\n",
    "        ---------\n",
    "        X : {array-like, sparse matrix} o kształcie (n_próbek, n_cech)\n",
    "            Próbki.\n",
    "            \n",
    "        Zwraca\n",
    "        -------\n",
    "        y : array o kształcie (n_próbek,)\n",
    "            Przewidziane wartości.\n",
    "        \"\"\"\n",
    "        # Sprawdzenie czy model został wytrenowany\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        \n",
    "        # Sprawdzenie poprawności X\n",
    "        X = check_array(X)\n",
    "        \n",
    "        # Przewidywanie\n",
    "        if self.fit_intercept:\n",
    "            return np.dot(X, self.coef_) + self.intercept_\n",
    "        else:\n",
    "            return np.dot(X, self.coef_)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Zwraca współczynnik determinacji R^2 predykcji.\n",
    "        \n",
    "        Parametry\n",
    "        ---------\n",
    "        X : array-like o kształcie (n_próbek, n_cech)\n",
    "            Próbki testowe.\n",
    "        y : array-like o kształcie (n_próbek,)\n",
    "            Prawdziwe wartości dla X.\n",
    "            \n",
    "        Zwraca\n",
    "        -------\n",
    "        score : float\n",
    "            R^2 predykcji.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 2)\n",
    "y = 3 * X[:, 0] + 5 * X[:, 1] + np.random.randn(100) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Współczynnik determinacji R^2 (sklearn): 0.9085743526951848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_sklearn = LinearRegression()\n",
    "model_sklearn.fit(X, y)\n",
    "predictions_sklearn = model_sklearn.predict(X)\n",
    "print(\"Współczynnik determinacji R^2 (sklearn):\", model_sklearn.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potoki (*Pipeline*)\n",
    "\n",
    "Potoki to mechanizm, który umożliwia łączenie wielu kroków przetwarzania danych w jedną spójną jednostkę. \n",
    "\n",
    "### Podstawowa idea\n",
    "\n",
    "Potok w scikit-learn pozwala na sekwencyjne łączenie różnych transformatorów i jednego estymatora końcowego w jedną całość. Zapewnia to:\n",
    "\n",
    "1. **Spójny interfejs** - cały potok zachowuje się jak pojedynczy estymator scikit-learn z metodami `fit()`, `predict()`, itd.\n",
    "2. **Automatyczne przekazywanie danych** - dane są automatycznie przekazywane między etapami\n",
    "3. **Eliminację wycieków danych** - potok zapewnia, że transformacje (np. skalowanie) są dopasowywane tylko do danych treningowych\n",
    "\n",
    "## Tworzenie potoku\n",
    "\n",
    "Podstawowy potok tworzy się przy użyciu klasy `Pipeline`:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Cały potok można trenować jak pojedynczy estymator\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# I używać do predykcji\n",
    "predictions = pipe.predict(X_test)\n",
    "```\n",
    "\n",
    "### Główne zalety potoków\n",
    "\n",
    "#### 1. Organizacja kodu\n",
    "\n",
    "Potoki organizują kod w zrozumiały i łatwy do utrzymania sposób - ważne, gdy łańcuch przetwarzania jest złożony.\n",
    "\n",
    "#### 2. Zapobieganie wyciekom danych\n",
    "\n",
    "Jednym z najważniejszych zastosowań potoków jest zapobieganie wyciekom danych podczas preprocesingu:\n",
    "\n",
    "Niepoprawne podejście (wyciek informacji):\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Skalowanie przed kroswalidacją - BŁĄD!\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Wyciek danych, bo skalujemy całość\n",
    "\n",
    "# Kroswalidacja\n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X_scaled, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "```\n",
    "Poprawne podejście z potokiem:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Tworzymy pipeline, który najpierw skaluje dane, a potem trenuje model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Kroswalidacja - skalowanie odbywa się osobno w każdym foldzie!\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "print(\"RMSE dla każdego folda:\", -scores)\n",
    "print(\"Średni RMSE:\", -scores.mean())\n",
    "\n",
    "```\n",
    "\n",
    "### 3. Optymalizacja hiperparametrów\n",
    "\n",
    "Potoki są szczególnie przydatne w połączeniu z technikami przeszukiwania hiperparametrów:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'scaler__with_mean': [True, False],  # parametry dla transformatora\n",
    "    'classifier__C': [0.1, 1.0, 10.0]    # parametry dla klasyfikatora\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, cv=5)\n",
    "search.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Zauważ, że parametry dla poszczególnych kroków potoku mają format: `nazwa_kroku__nazwa_parametru`.\n",
    "\n",
    "### 4. Serializacja i wdrażanie\n",
    "\n",
    "Cały potok można zapisać jako jeden obiekt:\n",
    "\n",
    "```python\n",
    "from joblib import dump, load\n",
    "\n",
    "# Zapisz cały potok\n",
    "dump(pipe, 'model_pipeline.joblib')\n",
    "\n",
    "# Załaduj go później\n",
    "loaded_pipe = load('model_pipeline.joblib')\n",
    "```\n",
    "\n",
    "## Zaawansowane funkcje potoków\n",
    "\n",
    "### ColumnTransformer\n",
    "\n",
    "`ColumnTransformer` umożliwia zastosowanie różnych transformacji do różnych kolumn:\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['wiek', 'dochod']),\n",
    "        ('cat', OneHotEncoder(), ['kategoria', 'region'])\n",
    "    ])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "```\n",
    "\n",
    "### FeatureUnion\n",
    "\n",
    "`FeatureUnion` pozwala na równoległe przetwarzanie danych przez różne transformatory:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "features = FeatureUnion([\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('select_best', SelectKBest(k=2))\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('features', features),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
